\chapter{Results}
%\label{chapter:title}

\section{K-Means}

\subsection{Implementation}

\begin{wrapfigure}[25]{L}{0.4\textwidth}
    \centering
    \resizebox{0.3\textwidth}{!}{
        \begin{tikzpicture}[node distance=2cm]
            \node (in1) [io] {Load points to DPU memory};
            \node (pro1c) [io, below=\vspacing of in1] {Broadcast centroids to all DPUs};
            \node (pro1) [processdpu, below=\vspacing of pro1c] {Initialize cluster sums and cluster counters to 0};
            \node (pro2) [processdpu, below=\vspacing of pro1] {For each point find the nearest centroid};
            \node (pro3) [processdpu, below=\vspacing of pro2] {For each point add features to the appropriate cluster sum and increment the cluster counter};
            \node (pro2c) [processcpu, below=\vspacing of pro3] {Recover and sum the cluster sums and counters from all DPUs};
            \node (pro3c) [processcpu, below=\vspacing of pro2c] {Average the new centroids of clusters};
            \node (dec1) [decision, below=\vspacing of pro3c] {Centroids changed?};
            \node (out1) [stop, below=\vspacing of dec1] {Output centroids};

            \draw [arrow] (in1) -- (pro1c);
            \draw [arrow] (pro1c) -- (pro1);
            \draw [arrow] (pro1) -- (pro2);
            \draw [arrow] (pro2) -- (pro3);
            \draw [arrow] (pro3) -- (pro2c);
            \draw [arrow] (pro2c) -- (pro3c);
            \draw [arrow] (pro3c) -- (dec1);
            \draw [arrow] (dec1) -- node[anchor=east] {no} (out1);
            \draw [arrow] (dec1.east) -- node[anchor=south] {yes} ++(3,0) |- (pro1c.east);

            \background{pro1}{pro1}{pro3}{pro3}{I}
        \end{tikzpicture}
    }
    \caption{\label{fig:KMeansDPU}The K-Means algorithm on DPU}
\end{wrapfigure}

The main workload in K-Means is to measure pairwise distances between each point-centroid pair. Obviously this would be way too slow in floating point arithmetic with DPUs. My approach is instead to do a linear quantization of the data on 15 bits. That is to say the features are mapped linearly to the range [-16385,16384] and encoded as 16-bit integers. The extra bit of leeway is there so that we don't overflow when subtracting features. The entire algorithm is then run in this format, and only at the end the quantized data is converted back to floating point.

This brings two questions. The first one is why choose 16 bits and not 8, since the hardware supports only 8-bit multiplications? Wouldn't that mean that we would end up being 4 times slower than in 8 bits, since performing a 16 bits multiplication with 8 bits hardware needs 4 multiplications? To understand this choice, remember that the DPUs are RISC processors. This means that for each cycle they only execute a very simple instruction. For example, a multiplication needs two load instructions (one for each operand), one multiplication instruction, and one store instruction.

Consider the following simple function to compute the squared euclidean distance between two vectors:
\begin{lstlisting}[language=C]
#include <stdint.h>

int64_t euclid(int16_t* a, int16_t* b, uint32_t size) {
    int64_t accumulate;
    for(uint32_t i=0; i<size; i++) {
        volatile int16_t diff = a[i] - b[i];
        accumulate += diff * diff;
    }
    return accumulate;
}    
\end{lstlisting}

Once compiled for DPUs, this function ends up being 26 instructions long. Only 12 instructions are spent on the actual multiplication, and the rest is spent on the subtraction, the loop and the return. If we replace the inputs and the difference with 8-bits integers, the function is now 17 instructions long. All in all, that's only a 60\% performance degradation for more than double the numerical precision, which is quite the trade-off.

The other question is can we trust the algorithm once we've quantized the data? Surely, it is possible to construct a counter-example where the quantization would lead to an incorrect result. However, and allow me to emphasize this, \textbf{floating point K-Means has similar issues, especially in 32 bits}~\cite{jezequel:hal-02486753}. Working in floating point isn't a guarantee against rounding errors, quite the opposite. Working with integers, we can at least be sure that small values don't get rounded to zero when we perform long addition loops.

The important question isn't theoretical, but empirical: does the algorithm generally work on real datasets? We will see that it does.